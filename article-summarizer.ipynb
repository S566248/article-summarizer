{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Final Project: Article Summarizer\n",
    "\n",
    "### Student Name: Tyler Stanton\n",
    "\n",
    "#### GitHub Repo: https://github.com/S566248/article-summarizer\n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "You should bring in code from previous assignments to help you answer the questions below.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the imports I am expecting to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version\n",
      "------------------ -----------\n",
      "annotated-types    0.6.0\n",
      "asttokens          2.4.1\n",
      "beautifulsoup4     4.12.3\n",
      "blis               0.7.11\n",
      "catalogue          2.0.10\n",
      "certifi            2024.2.2\n",
      "charset-normalizer 3.3.2\n",
      "click              8.1.7\n",
      "cloudpathlib       0.16.0\n",
      "colorama           0.4.6\n",
      "comm               0.2.2\n",
      "confection         0.1.4\n",
      "contourpy          1.2.1\n",
      "cycler             0.12.1\n",
      "cymem              2.0.8\n",
      "debugpy            1.8.1\n",
      "decorator          5.1.1\n",
      "executing          2.0.1\n",
      "fonttools          4.51.0\n",
      "idna               3.7\n",
      "ipykernel          6.29.4\n",
      "ipython            8.23.0\n",
      "jedi               0.19.1\n",
      "Jinja2             3.1.3\n",
      "jupyter_client     8.6.1\n",
      "jupyter_core       5.7.2\n",
      "kiwisolver         1.4.5\n",
      "langcodes          3.3.0\n",
      "MarkupSafe         2.1.5\n",
      "matplotlib         3.8.4\n",
      "matplotlib-inline  0.1.7\n",
      "murmurhash         1.0.10\n",
      "nest-asyncio       1.6.0\n",
      "numpy              1.26.4\n",
      "packaging          24.0\n",
      "parso              0.8.4\n",
      "pillow             10.3.0\n",
      "pip                24.0\n",
      "platformdirs       4.2.0\n",
      "preshed            3.0.9\n",
      "prompt-toolkit     3.0.43\n",
      "psutil             5.9.8\n",
      "pure-eval          0.2.2\n",
      "pydantic           2.7.0\n",
      "pydantic_core      2.18.1\n",
      "Pygments           2.17.2\n",
      "pyparsing          3.1.2\n",
      "python-dateutil    2.9.0.post0\n",
      "pywin32            306\n",
      "pyzmq              26.0.0\n",
      "requests           2.31.0\n",
      "setuptools         69.5.1\n",
      "six                1.16.0\n",
      "smart-open         6.4.0\n",
      "soupsieve          2.5\n",
      "spacy              3.7.4\n",
      "spacy-legacy       3.0.12\n",
      "spacy-loggers      1.0.5\n",
      "srsly              2.4.8\n",
      "stack-data         0.6.3\n",
      "thinc              8.2.3\n",
      "tornado            6.4\n",
      "tqdm               4.66.2\n",
      "traitlets          5.14.2\n",
      "typer              0.9.4\n",
      "typing_extensions  4.11.0\n",
      "urllib3            2.2.1\n",
      "wasabi             1.1.2\n",
      "wcwidth            0.2.13\n",
      "weasel             0.3.4\n",
      "All prereqs installed.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "import requests\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "!pip list\n",
    "\n",
    "print('All prereqs installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find on the internet an article or blog post about a topic that interests you and you are able to get the text for using the technologies we have applied in the course.  Get the html for the article and store it in a file (which you must submit with your project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content has been successfully saved to 'article_html.html'\n"
     ]
    }
   ],
   "source": [
    "url = \"https://reflector.uindy.edu/2024/04/10/the-caitlin-clark-effect-how-the-iowa-star-is-changing-womens-sports/\"\n",
    "\n",
    "# Send a GET request to the URL to fetch the HTML content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Get the HTML content\n",
    "    html_content = response.text\n",
    "\n",
    "    # Write the HTML content to a file\n",
    "    with open(\"article_html.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(html_content)\n",
    "        print(\"HTML content has been successfully saved to 'article_html.html'\")\n",
    "else:\n",
    "    print(\"Failed to fetch HTML content from the URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read in your article's html source from the file you created in question 1 and do sentiment analysis on the article/post's text (use `.get_text()`).  Print the polarity score with an appropriate label.  Additionally print the number of sentences in the original article (with an appropriate label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Read the HTML source from the file\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle_html.html\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Read the HTML source from the file\n",
    "with open(\"article_html.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Extract the text from the article\n",
    "article_text = soup.get_text()\n",
    "\n",
    "# Perform sentiment analysis on the article's text\n",
    "blob = TextBlob(article_text)\n",
    "polarity_score = blob.sentiment.polarity\n",
    "\n",
    "# Print the polarity score with an appropriate label\n",
    "print(\"Polarity Score:\", polarity_score)\n",
    "\n",
    "# Count the number of sentences in the original article\n",
    "sentences = blob.sentences\n",
    "num_sentences = len(sentences)\n",
    "\n",
    "# Print the number of sentences with an appropriate label\n",
    "print(\"Number of Sentences:\", num_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the article text using spaCy\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Get all lowercase tokens from the processed text\n",
    "tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "# Count the frequency of each token\n",
    "token_freq = Counter(tokens)\n",
    "\n",
    "# Get the 5 most frequent tokens\n",
    "common_tokens = token_freq.most_common(5)\n",
    "\n",
    "# Print the common tokens with their frequencies\n",
    "print(\"Common Tokens:\")\n",
    "for token, freq in common_tokens:\n",
    "    print(f\"{token}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the article text using spaCy\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Get all lowercase lemmas from the processed text\n",
    "lemmas = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "# Count the frequency of each lemma\n",
    "lemma_freq = Counter(lemmas)\n",
    "\n",
    "# Get the 5 most frequent lemmas\n",
    "common_lemmas = lemma_freq.most_common(5)\n",
    "\n",
    "# Print the common lemmas with their frequencies\n",
    "print(\"Common Lemmas:\")\n",
    "for lemma, freq in common_lemmas:\n",
    "    print(f\"{lemma}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the article text using spaCy\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Calculate the sentiment score for each sentence using tokens\n",
    "sentence_scores = []\n",
    "for sentence in doc.sents:\n",
    "    # Get the tokens in the sentence\n",
    "    tokens = [token.text.lower() for token in sentence if not token.is_stop and not token.is_punct]\n",
    "    # Calculate the score based on the number of tokens\n",
    "    score = sum(1 for token in tokens if token in positive_words) - sum(1 for token in tokens if token in negative_words)\n",
    "    sentence_scores.append(score)\n",
    "\n",
    "# Plot a histogram of the scores\n",
    "plt.hist(sentence_scores, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Sentence Sentiment Scores')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Comment: The most common range of scores seems to be around 0, indicating neutral sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the article text using spaCy\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Calculate the sentiment score for each sentence using lemmas\n",
    "sentence_scores = []\n",
    "for sentence in doc.sents:\n",
    "    # Get the lemmas in the sentence\n",
    "    lemmas = [token.lemma_.lower() for token in sentence if not token.is_stop and not token.is_punct]\n",
    "    # Calculate the score based on the number of lemmas\n",
    "    score = sum(1 for lemma in lemmas if lemma in positive_lemmas) - sum(1 for lemma in lemmas if lemma in negative_lemmas)\n",
    "    sentence_scores.append(score)\n",
    "\n",
    "# Plot a histogram of the scores\n",
    "plt.hist(sentence_scores, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Sentence Sentiment Scores (using Lemmas)')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Comment: The most common range of scores seems to be around 0, indicating neutral sentiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using the histograms from questions 5 and 6, decide a \"cutoff\" score for tokens and lemmas such that fewer than half the sentences would have a score greater than the cutoff score.  Record the scores in this Markdown cell\n",
    "\n",
    "* Cutoff Score (tokens): \n",
    "* Cutoff Score (lemmas):\n",
    "\n",
    "Feel free to change these scores as you generate your summaries.  Ideally, we're shooting for at least 6 sentences for our summary, but don't want more than 10 (these numbers are rough estimates; they depend on the length of your article)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on tokens) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cutoff score\n",
    "cutoff_score = 0  # Adjust this value based on your findings in question 8\n",
    "\n",
    "# Create an empty list to store selected sentences\n",
    "summary_sentences = []\n",
    "\n",
    "# Iterate through every sentence in the article\n",
    "for sentence in doc.sents:\n",
    "    # Calculate the sentiment score for the sentence using tokens\n",
    "    score = score_sentence_by_token(sentence, most_common_tokens)\n",
    "    # Check if the score is greater than the cutoff\n",
    "    if score > cutoff_score:\n",
    "        # Add the sentence to the summary\n",
    "        summary_sentences.append(sentence.text.strip())\n",
    "\n",
    "# Join the selected sentences into a summary\n",
    "summary_text = ' '.join(summary_sentences)\n",
    "\n",
    "# Print the summary\n",
    "print(summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Print the polarity score of your summary you generated with the token scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the summary text into a spaCy pipeline\n",
    "summary_doc = nlp(summary_text)\n",
    "\n",
    "# Calculate the sentiment polarity score of the summary\n",
    "summary_polarity_score = get_sentiment_polarity(summary_doc)\n",
    "\n",
    "# Print the polarity score of the summary with an appropriate label\n",
    "print(\"Polarity Score of the Summary (Token Scores):\", summary_polarity_score)\n",
    "\n",
    "# Count the number of sentences in the summarized article\n",
    "num_sentences_summary = len(list(summary_doc.sents))\n",
    "\n",
    "# Print the number of sentences in the summarized article with an appropriate label\n",
    "print(\"Number of Sentences in the Summarized Article:\", num_sentences_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on lemmas) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store sentences that meet the cutoff score criteria\n",
    "summary_sentences = []\n",
    "\n",
    "# Iterate through every sentence in the article\n",
    "for sent in article_doc.sents:\n",
    "    # Calculate the score for each sentence based on lemmas\n",
    "    sentence_score = score_sentence_by_lemma(sent, common_lemmas)\n",
    "    # Check if the score is greater than the cutoff score identified in question 8\n",
    "    if sentence_score > cutoff_score:\n",
    "        # Add the sentence to the list\n",
    "        summary_sentences.append(sent.text.strip())\n",
    "\n",
    "# Join the sentences in the list to generate the summary text\n",
    "summary_text = ' '.join(summary_sentences)\n",
    "\n",
    "# Print the summary text\n",
    "print(\"Summary based on Lemmas:\")\n",
    "print(summary_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Print the polarity score of your summary you generated with the lemma scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "To print the polarity score of the summary generated with the lemma scores and the number of sentences in the summarized article, you can follow these steps:\n",
    "\n",
    "Calculate the polarity score of the summary text based on lemma scores.\n",
    "Count the number of sentences in the summarized article.\n",
    "Print the polarity score and the number of sentences with appropriate labels.\n",
    "Here's how you can implement it in Python:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Calculate the polarity score of the summary text based on lemma scores\n",
    "summary_polarity = get_polarity_score(summary_text, lemma_scores)\n",
    "\n",
    "# Count the number of sentences in the summarized article\n",
    "num_sentences_summary = len(summary_sentences)\n",
    "\n",
    "# Print the polarity score of the summary with an appropriate label\n",
    "print(\"Polarity score of the summary (based on Lemmas):\", summary_polarity)\n",
    "\n",
    "# Print the number of sentences in the summarized article with an appropriate label\n",
    "print(\"Number of sentences in the summarized article:\", num_sentences_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.  Compare your polarity scores of your summaries to the polarity scores of the initial article.  Is there a difference?  Why do you think that may or may not be?.  Answer in this Markdown cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Based on your reading of the original article, which summary do you think is better (if there's a difference).  Why do you think this might be?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
